{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c197734c-ce0d-4e65-8c80-e2977b6ad0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nltk in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/hrishityelchuri/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd237c3d-8436-410c-85ee-7f919889a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk_utils import bag_of_words, tokenize, stem\n",
    "from model import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "316a5728-0802-4c9a-a22f-cbd20dd14b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hrishityelchuri/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b46724-d1ab-420c-8560-67e013af6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c58ce3f-9e04-4c8d-9b3b-33258fcdf90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "xy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364a4c74-7722-4458-823c-8d48c0300520",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    \n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99648c91-0444-4022-a78d-bb4ed998d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 patterns\n",
      "38 tags: ['admission', 'canteen', 'college intake', 'committee', 'computerhod', 'course', 'creator', 'document', 'event', 'extchod', 'facilities', 'fees', 'floors', 'goodbye', 'greeting', 'hod', 'hostel', 'hours', 'infrastructure', 'ithod', 'library', 'location', 'menu', 'name', 'number', 'placement', 'principal', 'ragging', 'random', 'salutaion', 'scholarship', 'sem', 'sports', 'swear', 'syllabus', 'task', 'uniform', 'vacation']\n",
      "250 unique stemmed words: [\"'s\", '(', ')', 'a', 'about', 'ac', 'activ', 'address', 'admis', 'admiss', 'against', 'ai/ml', 'allot', 'am', 'an', 'and', 'ani', 'antirag', 'anyon', 'are', 'ass', 'asshol', 'at', 'attend', 'automobil', 'avail', 'averag', 'be', 'between', 'big', 'bitch', 'book', 'boy', 'branch', 'bring', 'build', 'by', 'bye', 'cafetaria', 'call', 'campu', 'can', 'canteen', 'capac', 'case', 'casual', 'ce', 'chat', 'chemic', 'civil', 'code', 'colleg', 'come', 'committ', 'committe', 'comp', 'compani', 'comput', 'conduct', 'contact', 'cours', 'creat', 'creator', 'cya', 'date', 'day', 'design', 'detail', 'develop', 'differ', 'distanc', 'do', 'document', 'doe', 'done', 'dress', 'dresscod', 'dumb', 'dure', 'each', 'eat', 'end', 'engin', 'event', 'exam', 'extc', 'facil', 'far', 'fee', 'first', 'floor', 'food', 'for', 'fourth', 'from', 'fuck', 'fucker', 'function', 'game', 'get', 'girl', 'give', 'go', 'good', 'goodby', 'got', 'gtg', 'guy', 'have', 'held', 'hell', 'hello', 'help', 'here', 'heyi', 'hi', 'histori', 'hod', 'holiday', 'hostel', 'hour', 'how', 'i', 'idiot', 'in', 'incid', 'info', 'inform', 'infrastructur', 'intak', 'is', 'it', 'job', 'junior', 'k', 'later', 'leav', 'lectur', 'librari', 'list', 'locat', 'long', 'love', 'made', 'mani', 'marri', 'max', 'maximum', 'me', 'mechan', 'menu', 'more', 'much', 'my', 'name', 'need', 'next', 'nice', 'no', 'non-ac', 'number', 'of', 'offer', 'offic', 'ok', 'okay', 'oki', 'okk', 'on', 'open', 'oper', 'organis', 'packag', 'per', 'phone', 'placement', 'pleas', 'practic', 'princip', 'process', 'provid', 'rag', 'reach', 'recruit', 'requir', 'room', 'saturday', 'schedul', 'scholarship', 'seat', 'second', 'see', 'sem', 'semest', 'serviv', 'should', 'shut', 'size', 'someth', 'sport', 'start', 'student', 'stundent', 'stupid', 'syllabu', 'take', 'taken', 'talk', 'tall', 'technolog', 'telephon', 'tell', 'thank', 'the', 'there', 'these', 'thi', 'thing', 'third', 'time', 'timet', 'to', 'ttyl', 'u', 'uni', 'uniform', 'univrs', 'up', 'use', 'vacat', 'varieti', 'visit', 'we', 'wear', 'well', 'what', 'whatsup', 'whatv', 'when', 'where', 'whi', 'which', 'who', 'whom', 'will', 'work', 'ya', 'year', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(xy), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_words), \"unique stemmed words:\", all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02a24344-b908-455e-a39d-b852bb717dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 38\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    \n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    \n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "num_epochs = 2000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6babfe93-c76d-49c4-902c-9defb6b26362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "   \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b6a8719-8c93-4e23-9627-9de74b2f716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20134e5c-2d89-4c06-8232-70142306ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70625bd9-7645-4b10-b54c-e59f73218ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a9e1a0-2198-4097-a4e5-dd7a11ecbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e57ba54b-d76b-49ac-b080-3ed1404a173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 0.1044\n",
      "Epoch [200/2000], Loss: 0.0011\n",
      "Epoch [300/2000], Loss: 0.0003\n",
      "Epoch [400/2000], Loss: 0.0000\n",
      "Epoch [500/2000], Loss: 0.0000\n",
      "Epoch [600/2000], Loss: 0.0000\n",
      "Epoch [700/2000], Loss: 0.0000\n",
      "Epoch [800/2000], Loss: 0.0000\n",
      "Epoch [900/2000], Loss: 0.0000\n",
      "Epoch [1000/2000], Loss: 0.0000\n",
      "Epoch [1100/2000], Loss: 0.0000\n",
      "Epoch [1200/2000], Loss: 0.0000\n",
      "Epoch [1300/2000], Loss: 0.0004\n",
      "Epoch [1400/2000], Loss: 0.0000\n",
      "Epoch [1500/2000], Loss: 0.0000\n",
      "Epoch [1600/2000], Loss: 0.0000\n",
      "Epoch [1700/2000], Loss: 0.0000\n",
      "Epoch [1800/2000], Loss: 0.0000\n",
      "Epoch [1900/2000], Loss: 0.0000\n",
      "Epoch [2000/2000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        \n",
    "        outputs = model(words)\n",
    "       \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f40ac8d5-1ac5-4a09-9d27-d1772bf554bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "177145bd-7f01-404a-a658-91c25f5ce4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac70906-a80a-4f7e-af10-22d02562bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
